{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8fcfe2d-f79a-47ae-a79d-8ef899bffe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are trying to extract 2024 weather data for our 2024 sample data set\n",
    "# to test whether the weather data extraction can be done successfully \n",
    "# we are gonna use dataset \"2024_weather_data_with_coords_complete.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c02781-580c-44c6-afbe-a5da8be61e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8701e2b-497f-47a1-b731-9535f217398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # this allows nested async loops\n",
    "\n",
    "class WeatherDataProcessor:\n",
    "    def __init__(self, api_key='023826bb6ba471680c4069db1a508424', cache_db='weather_cache.db', max_concurrent=5):\n",
    "        self.api_key = api_key\n",
    "        self.cache_db = cache_db\n",
    "        self.max_concurrent = max_concurrent\n",
    "        self.setup_cache()\n",
    "        self.session = None\n",
    "    \n",
    "    def setup_cache(self):\n",
    "        with sqlite3.connect(self.cache_db) as conn:\n",
    "            conn.execute('''\n",
    "                CREATE TABLE IF NOT EXISTS weather_cache (\n",
    "                    location_date TEXT PRIMARY KEY,\n",
    "                    weather_data TEXT,\n",
    "                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP\n",
    "                )\n",
    "            ''')\n",
    "            conn.execute('CREATE INDEX IF NOT EXISTS idx_location_date ON weather_cache(location_date)')\n",
    "\n",
    "    async def get_weather_data(self, lat, lon, date_str):\n",
    "        cache_key = f\"{lat}_{lon}_{date_str}\"\n",
    "        \n",
    "        # check cache first\n",
    "        cached_weather = self._get_cached_weather(cache_key)\n",
    "        if cached_weather:\n",
    "            return cached_weather\n",
    "\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        timestamp = int(date.timestamp())\n",
    "        \n",
    "        url = \"https://history.openweathermap.org/data/2.5/history/city\"\n",
    "        params = {\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'type': 'hour',\n",
    "            'start': timestamp,\n",
    "            'end': timestamp + 86400,\n",
    "            'appid': self.api_key,\n",
    "            'units': 'metric'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            async with self.session.get(url, params=params) as response:\n",
    "                if response.status == 200:\n",
    "                    data = await response.json()\n",
    "                    if data.get('list'):\n",
    "                        weather_data = self._process_weather_data(data)\n",
    "                        if weather_data:\n",
    "                            self._cache_weather(cache_key, weather_data)\n",
    "                        return weather_data\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching weather data: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    async def process_batch(self, batch_df):\n",
    "        if self.session is None:\n",
    "            self.session = aiohttp.ClientSession()\n",
    "\n",
    "        weather_columns = ['temperature', 'humidity', 'wind_speed', \n",
    "                         'clouds', 'precipitation', 'pressure',\n",
    "                         'weather_description', 'data_type']\n",
    "        \n",
    "        semaphore = asyncio.Semaphore(self.max_concurrent)\n",
    "        \n",
    "        async def process_row(row):\n",
    "            async with semaphore:\n",
    "                date_str = row['date'].strftime('%Y-%m-%d')\n",
    "                weather_data = await self.get_weather_data(row['latitude'], row['longitude'], date_str)\n",
    "                await asyncio.sleep(0.2)  # Rate limiting\n",
    "                return row.name, weather_data\n",
    "\n",
    "        tasks = [process_row(row) for _, row in batch_df.iterrows()]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        for idx, weather_data in results:\n",
    "            if weather_data:\n",
    "                for col in weather_columns:\n",
    "                    batch_df.loc[idx, col] = weather_data.get(col)\n",
    "        \n",
    "        return batch_df\n",
    "\n",
    "    async def process_all_data(self, df, batch_size):\n",
    "        processed_dfs = []\n",
    "        \n",
    "        with tqdm(total=len(df), desc=\"Processing\", unit=\"records\") as pbar:\n",
    "            for start_idx in range(0, len(df), batch_size):\n",
    "                end_idx = min(start_idx + batch_size, len(df))\n",
    "                batch = df.iloc[start_idx:end_idx].copy()\n",
    "                \n",
    "                processed_batch = await self.process_batch(batch)\n",
    "                processed_dfs.append(processed_batch)\n",
    "                \n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "                # save interim results incase any unexpected things happeen\n",
    "                if len(processed_dfs) % 10 == 0:\n",
    "                    interim_df = pd.concat(processed_dfs)\n",
    "                    interim_df.to_csv(f'weather_data_interim_{len(processed_dfs)}.csv', index=False)\n",
    "        \n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "            self.session = None\n",
    "            \n",
    "        return pd.concat(processed_dfs)\n",
    "\n",
    "    def process_by_date_range(self, input_file, start_date='2024-02-02', end_date='2025-02-01', batch_size=50):\n",
    "        print(f\"\\nProcessing date range: {start_date} to {end_date}\")\n",
    "        \n",
    "        # read and prepare data\n",
    "        df = pd.read_csv(input_file)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # filter date range\n",
    "        mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
    "        df = df[mask].copy()\n",
    "        \n",
    "        total_records = len(df)\n",
    "        print(f\"\\nTotal records to process: {total_records}\")\n",
    "        \n",
    "        estimated_hours = (total_records * 0.2) / 3600\n",
    "        print(f\"Estimated processing time: {estimated_hours:.1f} hours\\n\")\n",
    "\n",
    "        # process the data using asyncio\n",
    "        loop = asyncio.get_event_loop()\n",
    "        result_df = loop.run_until_complete(self.process_all_data(df, batch_size))\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def _process_weather_data(self, api_response):\n",
    "        try:\n",
    "            if not api_response.get('list'):\n",
    "                return None\n",
    "                \n",
    "            daily_data = api_response['list'][0]\n",
    "            weather_info = daily_data.get('weather', [{}])[0]\n",
    "            \n",
    "            return {\n",
    "                'temperature': daily_data.get('main', {}).get('temp'),\n",
    "                'feels_like': daily_data.get('main', {}).get('feels_like'),\n",
    "                'humidity': daily_data.get('main', {}).get('humidity'),\n",
    "                'pressure': daily_data.get('main', {}).get('pressure'),\n",
    "                'wind_speed': daily_data.get('wind', {}).get('speed'),\n",
    "                'clouds': daily_data.get('clouds', {}).get('all'),\n",
    "                'precipitation': daily_data.get('rain', {}).get('1h', 0),\n",
    "                'weather_description': weather_info.get('description'),\n",
    "                'data_type': 'historical'\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing weather data: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def _get_cached_weather(self, cache_key):\n",
    "        with sqlite3.connect(self.cache_db) as conn:\n",
    "            result = conn.execute(\n",
    "                \"SELECT weather_data FROM weather_cache WHERE location_date = ?\",\n",
    "                (cache_key,)\n",
    "            ).fetchone()\n",
    "            return json.loads(result[0]) if result else None\n",
    "\n",
    "    def _cache_weather(self, cache_key, weather_data):\n",
    "        with sqlite3.connect(self.cache_db) as conn:\n",
    "            conn.execute(\n",
    "                \"INSERT OR REPLACE INTO weather_cache (location_date, weather_data) VALUES (?, ?)\",\n",
    "                (cache_key, json.dumps(weather_data))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e484b02a-1e6b-404d-ab3d-afffc15555d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing date range: 2024-02-05 to 2025-02-01\n",
      "\n",
      "Total records to process: 28314\n",
      "Estimated processing time: 1.6 hours\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|████████████████████| 28314/28314 [44:18<00:00, 10.65records/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize the processor\n",
    "processor = WeatherDataProcessor()\n",
    "\n",
    "# process the data\n",
    "result_df = processor.process_by_date_range(\n",
    "    input_file='2024_weather_data_with_coords_complete.csv',\n",
    "    start_date='2024-02-05',\n",
    "    end_date='2025-02-01',\n",
    "    batch_size=50  # Optimized batch size\n",
    ")\n",
    "\n",
    "# save the final results\n",
    "result_df.to_csv('2024_weather_data_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca302a-99c8-4271-bf97-a92094528276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce2f65b-8aa4-4600-9b79-a6fb1fdf117e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
